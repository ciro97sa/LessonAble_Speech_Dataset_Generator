wavs/20211022_154820_0.wav|Few words about the evaluation of numeric prediction.
wavs/20211022_154820_2.wav|Obviously, there are other measures.
wavs/20211022_154820_3.wav|I can use also the mean absolute error.
wavs/20211022_154820_5.wav|The main problem is how to choose a specific value.
wavs/20211022_154820_7.wav|Well, in this case, it is quite difficult to say that one measure is better than another one.
wavs/20211022_154820_9.wav|Correlation coefficient measure the statistical correlation between predicted values and actual values.
wavs/20211022_154820_11.wav|In this case, the higher is better.
wavs/20211022_154820_12.wav|For correlation coefficient, zero point nine one is better than all the other value.
wavs/20211022_154820_13.wav|While in the other case, the lower is better.
wavs/20211022_154820_16.wav|In case of A and B, it's also simple to say that one algorithm is better than another one.
wavs/20211022_154820_17.wav|For example, A is better for this parameter, B is better for this parameter.
wavs/20211022_154820_19.wav|But anyway, we can use it in case of classification.
wavs/20211022_154820_20.wav|In case of classification, the first is related to a classifier.
wavs/20211022_154820_21.wav|It accounts for the complexity of a classifier.
wavs/20211022_154820_22.wav|We want to find the best possible tradeoff between complexity and accuracy.
wavs/20211022_154820_23.wav|If two classifiers have the same complexity, I prefer the one which has the maximum possible performance.
wavs/20211022_154820_24.wav|We want a minimum description length.
wavs/20211022_154820_25.wav|We want to follow a minimal description length approach.
wavs/20211022_154820_26.wav|We want to select the classifier with the minimum description length.
wavs/20211022_154820_27.wav|We want to have a low complexity classifier with the highest possible performance.
wavs/20211022_154820_28.wav|If the performance are very high, errors are very low.
wavs/20211022_154820_29.wav|We want to minimize errors and minimize also the complexity.
wavs/20211022_154820_30.wav|Typically, if I augment the complexity, error decreases.
wavs/20211022_154820_31.wav|I want to find the best tradeoff between complexity and the number of errors.
wavs/20211022_154820_32.wav|I to minimize the sum of these two terms.
wavs/20211022_154820_33.wav|The Occam's razor says that the best theory is the smallest one that describes all the facts.
wavs/20211022_154820_34.wav|The smallest one means the simplest classifier with the lowest possible complexity.
wavs/20211022_154820_35.wav|The fact that it describes all the fact means that all the same polls have correctly classified.
wavs/20211022_154820_36.wav|In our words, this is how we can explain them.
wavs/20211022_154820_37.wav|How can I obtain these results?
wavs/20211022_154820_38.wav|In this case, the distance is the minimum possible.
wavs/20211022_154820_39.wav|But this case, the complexity is the maximum one.
wavs/20211022_154820_40.wav|We can consider as the description length of a theory.
wavs/20211022_154820_41.wav|The beats, for example, needed to encode the clusters.
wavs/20211022_154820_42.wav|We can have a measure which is directly proportional to the number of clusters.
wavs/20211022_154820_43.wav|This case, for example, I can run kmeans several times.
wavs/20211022_154820_44.wav|It's a possible application.
wavs/20211022_154820_45.wav|Obviously, less the distance is the better is the clustering solution.
wavs/20211022_154820_46.wav|The average distance is with respect to the centroid.
wavs/20211022_154820_47.wav|This ends this presentation.
wavs/20211022_154820_48.wav|We spend time about the problem of assessing performance.
wavs/20211022_154820_49.wav|Besides that, we should have learned how it is possible to correctly train a classifier.
wavs/20211022_154820_50.wav|How it is possible to correctly assess the performance of a classifier.
wavs/20211022_154820_51.wav|We can also use leaveoneout in some cases.
wavs/20211022_154820_52.wav|So validation set must be used for defining the value of the hyperparameters.
wavs/20211022_154820_53.wav|Don't forget this approach.
wavs/20211022_154820_54.wav|I need to properly assess the classifier, but also I need to probably train the classifier.
wavs/20211022_154820_55.wav|The standard algorithm, that I presented for building the decision tree to the so called Cfour point five.
wavs/20211022_154820_56.wav|Then in the next week, I'll have to present the these approaches.
wavs/20211022_154820_57.wav|Obviously, when you use a specific algorithm, you must know the meaning of this algorithm.
wavs/20211022_154820_58.wav|It's not correct to use an algorithm without knowing anything.
wavs/20211022_154820_59.wav|Obviously, this cannot be a good approach.
wavs/20211022_154820_60.wav|In some cases, it is possible to use also likely modification of our algorithm.
wavs/20211022_154820_61.wav|But anyway, you must be aware of these meters.
wavs/20211022_154820_62.wav|Not possible to use an approach without knowing anything about this approach.
wavs/20211022_154820_63.wav|It remains about fifteen minutes.
wavs/20211022_154820_64.wav|I'll start illustrating Cfour point five algorithm.
wavs/20211022_154820_65.wav|These considerations are quite general.
wavs/20211022_154820_66.wav|We have to extend the basic algorithms in order to cope with all these requirements.
wavs/20211022_154820_67.wav|It's not very simple, but it's possible obviously to cope with missing value.
wavs/20211022_154820_68.wav|The geometry can manage missing values.
wavs/20211022_154820_69.wav|Also, it is necessary to have an algorithm which is stable with respect to noisy data.
wavs/20211022_154820_70.wav|In this case, the solution is to use a pruning mechanism.
wavs/20211022_154820_71.wav|The first approach in fact is called the postpruning.
wavs/20211022_154820_72.wav|Postpruning is a meter that first build an entire tree and then prune it.
wavs/20211022_154820_73.wav|Postpruning, first build, then prune.
wavs/20211022_154820_74.wav|It's very wellknown.
wavs/20211022_154820_75.wav|There is also a commercial success, which is sixtyfive.
wavs/20211022_154820_76.wav|Cfour point five is the code.
wavs/20211022_154820_77.wav|Cfour point five is open source.
wavs/20211022_154820_78.wav|There are several implementation of Cfour point five algorithm.
wavs/20211022_154820_79.wav|One of them is the socalled the J fortyeight.
wavs/20211022_154820_80.wav|J fortyeight, four is related to the four of Cfour point five, eight hundred and fortyeight.
wavs/20211022_154820_81.wav|The eight is related to release eight.
wavs/20211022_154820_82.wav|J stands for Java.
wavs/20211022_154820_83.wav|J fortyeight, in other words is a particular implementation of this algorithm.
wavs/20211022_154820_84.wav|The work implementation of J fortyeight, is an implementation of the Cfour point five algorithm.
wavs/20211022_154820_85.wav|We can evaluate information gain.
wavs/20211022_154820_86.wav|The problems are always the same.
wavs/20211022_154820_87.wav|In decision tree algorithm, I have to choose the attribute for the first splitting.
wavs/20211022_154820_88.wav|In case of a nominal value, I illustrated which is the procedure.
wavs/20211022_154820_89.wav|How can I use a numeric attribute?
wavs/20211022_154820_90.wav|For example, if we consider as a threshold, seventyone point five.
wavs/20211022_154820_91.wav|One, two, three, four, yes and one, and two, No.
wavs/20211022_154820_92.wav|The value higher than seventyone point five, give us five Yes, and three No.
wavs/20211022_154820_93.wav|This case we have one, two, three, four, and five Yes, and one, two, and three No.
wavs/20211022_154820_94.wav|By considering all the possible split, I have to pick the best possible value.
wavs/20211022_154820_95.wav|In other words, I made sort those wipe after ordering the old values.
wavs/20211022_154820_96.wav|I forgot to say that in this case I've ordered all possible values for temperature then I made a sort those wipe by considering all the different point.
wavs/20211022_154820_97.wav|I stored the information gain for each point and I considered the best possible one.
wavs/20211022_154820_98.wav|This value is the information gain for the temperature attributes.
wavs/20211022_154820_99.wav|These only a bit costly from a computational point of view.
wavs/20211022_154820_100.wav|But anyway, it's very simple.
wavs/20211022_154820_101.wav|This case I have a information gain for temperature.
wavs/20211022_154820_102.wav|As I already said, I choose these procedure.
wavs/20211022_154820_103.wav|Says that if I have a missing value, I split instances with missing value into pieces.
wavs/20211022_154820_104.wav|Information gain can work also with fractional instances.
wavs/20211022_154820_105.wav|We have to use some weights instead of counts, but it performed in the same way.
wavs/20211022_154820_106.wav|I don't have no information about this example.
wavs/20211022_154820_107.wav|For example, if I'm considering a splitting in which half of the points are on the left and half of the points are on the right.
wavs/20211022_154820_108.wav|I weight zero point five zero point five.
wavs/20211022_154820_109.wav|They inaudible, otherwise, I'll do other waiting solution.
wavs/20211022_154820_110.wav|This is the procedure during training, during classification.
wavs/20211022_154820_111.wav|As we already said, I split the instance into pieces.
wavs/20211022_154820_112.wav|I can find the best probable class.
wavs/20211022_154820_113.wav|In this case typically, the most overall branch is related to the most probable class.
wavs/20211022_154820_114.wav|But anyway, it is possible that a single attribute is present more than once into the three.
wavs/20211022_154820_115.wav|This ends this lecture.
wavs/20211022_154820_116.wav|Have a nice weekend and see you next week.
wavs/20211022_154820_117.wav|I first stop recording.
wavs/20211022_144505_0.wav|So yeah we have to continue the discussion about the devaluation in particular.
wavs/20211022_144505_1.wav|I starting the presentation of an approach for comparing machine learning algorithm.
wavs/20211022_144505_2.wav|After that we have to finish the presentation by considering cost sensitive evaluation.
wavs/20211022_144505_3.wav|So in the lesson on Wednesday at least part of that lesson things.
wavs/20211022_144505_4.wav|This content may be also due to start contents related to the next chapter.
wavs/20211022_144505_5.wav|Just another few words.
wavs/20211022_144505_6.wav|And then you have to use this data in order to set up a machine learning business system.
wavs/20211022_144505_7.wav|But anyway at least in in all cases typically we try to give you tested data.
wavs/20211022_144505_8.wav|Okay, so we talked about the concept of confidence intervals.
wavs/20211022_144505_9.wav|When we compare learning schemes we should be able to say well, algorithm, A.
wavs/20211022_144505_10.wav|Is surely better than algorithm B for example.
wavs/20211022_144505_11.wav|Is able to say well classifier A is really better than classifier B.
wavs/20211022_144505_12.wav|We can say that yes.
wavs/20211022_144505_13.wav|We don't say this.
wavs/20211022_144505_14.wav|This point last time Typically bears that are used are ninety or ninetyfive.
wavs/20211022_144505_15.wav|Typically the confidence intervals are measured at the ninety or ninetyfive or confidence at the same time.
wavs/20211022_144505_16.wav|Infinitely made outside of it.
wavs/20211022_144505_17.wav|They specified the sides of the specific sides.
wavs/20211022_144505_18.wav|We can obtain an estimation by using the cross validation approach on each other.
wavs/20211022_144505_19.wav|Set for each scheme for for each algorithm.
wavs/20211022_144505_20.wav|Yeah, this in principle.
wavs/20211022_144505_21.wav|But in practice we don't have a very very huge data set.
wavs/20211022_144505_22.wav|We cannot simple many data sets of a predefined sides.
wavs/20211022_144505_23.wav|So, in this limited condition we can use some statistical tests in particular.
wavs/20211022_144505_24.wav|These assumption can cannot hold.
wavs/20211022_144505_25.wav|Okay, as we already made in case of interval confidence intervals.
wavs/20211022_144505_26.wav|So for example, if we consider ten estimates.
wavs/20211022_144505_27.wav|If we have ten estimates of the cross validation, we made ten across validations.
wavs/20211022_144505_28.wav|Yeah, Sorry, why wouldn't you?
wavs/20211022_144505_29.wav|Yeah, but but but there's a possibility.
wavs/20211022_144505_30.wav|But if I you if you use this value, it said typically it is very very difficult that classifier doesn't overlap.
wavs/20211022_144505_31.wav|It says that the difference must be significant at the eight level, ninety level ,ninetyfive level, okay.
wavs/20211022_144505_32.wav|This means that there is a one hundreda, chance that the true means differ, okay.
wavs/20211022_144505_33.wav|Now a, is in this case I have to consider in the table one hundred a, okay.
wavs/20211022_144505_34.wav|So, if we want to consider ninety have to consider ten divided by two, if there's five, okay.
wavs/20211022_144505_35.wav|This continues to be true, okay.
wavs/20211022_144505_36.wav|But anyway, the world data are also the same, so the union of all the folds are always the same data set, okay?
wavs/20211022_144505_37.wav|So, the statistics likely change and we can use these.
wavs/20211022_144505_38.wav|Okay so the procedure is exactly the same.
wavs/20211022_144505_39.wav|Typically it is possible that different errors have different costs.
wavs/20211022_144505_40.wav|For example, all cases of diagnosis, both full diagnosis and also medical diagnosis.
wavs/20211022_144505_41.wav|False alarm and misdetection can be also seen as false negative and false positive.
wavs/20211022_144505_42.wav|As you can see here is the actual class okay of a simple.
wavs/20211022_144505_43.wav|This is called the confusion matrix.
wavs/20211022_144505_44.wav|The confusion matrix we have a number or rates.
wavs/20211022_144505_45.wav|Obviously this can consider as true positive, same possible or true positive rates, possible motive rate, it doesn't matter.
wavs/20211022_144505_46.wav|This is a true positive if the actual class the real class is true, otherwise it is a false positive.
wavs/20211022_144505_47.wav|So a false positive analysis when the predicted class is yes while the actor class is no.
wavs/20211022_144505_48.wav|Okay, both positive is typically a false alarm.
wavs/20211022_144505_49.wav|So the classified says this is not a terrorist while this is a terrorist, okay.
wavs/20211022_144505_50.wav|Then we can assign different misclassification costs to false positive, false negative.
wavs/20211022_144505_51.wav|The most simple thing that I can do is simply to wait errors by means of discuss.
wavs/20211022_144505_52.wav|Okay, so what I want to say.
wavs/20211022_144505_53.wav|So in this case, as you can see, the cost for the errors are the same.
wavs/20211022_144505_54.wav|And then its performance must become considered as words with respect to another classifier.
wavs/20211022_144505_55.wav|The use of costs can be done also if the number of classes is different from from two.
wavs/20211022_144505_56.wav|Okay, so a custom matrics can be used in all cases, not only in case of binary classification.
wavs/20211022_144505_57.wav|Okay, so there's only a way for assessing performance.
wavs/20211022_144505_58.wav|But the output of the classifier is not influenced by the cost, okay?
wavs/20211022_144505_59.wav|I can evaluate an expected cost.
wavs/20211022_144505_60.wav|The expected cost is the dot product of the vector of class probability and a specific column in cost matrix.
wavs/20211022_144505_61.wav|Okay, so in this case the output for each class is given by this product.
wavs/20211022_144505_62.wav|Okay, I repeat this concept.
wavs/20211022_144505_63.wav|And then I want to, I have to say that the predicted class is that minimizes the expected cost.
wavs/20211022_144505_64.wav|The first case only by comparing classifiers.
wavs/20211022_144505_65.wav|So, learning is the same, classification is the same.
wavs/20211022_144505_66.wav|I use costs only to compare classifiers.
wavs/20211022_144505_67.wav|In second case I'm using costs for, in some cases modifying the output of a classifier.
wavs/20211022_144505_68.wav|In this case I'm using costs also for modifying the learning phase, the train, okay.
wavs/20211022_144505_69.wav|If my classifier outputs a probability vector, I can always use cost matrix.
wavs/20211022_144505_70.wav|In case of cost sensitive learning I have to modify the learning scheme.
wavs/20211022_144505_71.wav|So it's not so simple, it's not so general.
wavs/20211022_144505_72.wav|Here are some methods that can be used for cost sensitive learning.
wavs/20211022_144505_73.wav|I can resampling instances according to costs.
wavs/20211022_144505_74.wav|I can weighting instances according costs.
wavs/20211022_144505_75.wav|Obviously I can change the threshold.
wavs/20211022_144505_76.wav|Some symbols are classified on the other side, at the end all the symbol will be classified as belonging to the other class, okay.
wavs/20211022_144505_77.wav|So in this case, in other words I have a parameter by changing this parameter the performance of the system change, okay.
wavs/20211022_144505_78.wav|This can be done by using the so called ROC curves.
wavs/20211022_144505_79.wav|Okay, this is an example of ROC curves.
wavs/20211022_144505_80.wav|In case of ROC curves we plot false positive.
wavs/20211022_144505_81.wav|We can use zero, one hundred or one.
wavs/20211022_144505_82.wav|So this a rate but I can also use the value one.
wavs/20211022_144505_83.wav|Yeah, so, this is a the aerial evaluation for our classifier, okay?
wavs/20211022_144505_84.wav|Typically we have to have a curves that are over this line, okay?
wavs/20211022_144505_85.wav|On the other end, this one has been used by using just one set of testing.
wavs/20211022_144505_86.wav|Just one test set.
wavs/20211022_144505_87.wav|But anyway, a part of the fact that we can have a smoother or a cool that it's not particles move.
wavs/20211022_144505_88.wav|The important thing is that this could represent RO curve for receiver operating characteristic.
wavs/20211022_144505_89.wav|Which has been used in signal detection to show the trade off between it rate and false alarm rate over a noisy channel.
wavs/20211022_144505_90.wav|So this curve represent the behavior of the classifier as a specific parameter change.
wavs/20211022_144505_91.wav|And it's also possible to compare ROC curves, okay?
wavs/20211022_144505_92.wav|For example, A and B are different classifiers.
wavs/20211022_144505_93.wav|In this case, for example, A is the best classifier in this part of this plane.
wavs/20211022_144505_94.wav|In sense that this means that A is the best classifier if we want a small number of false positives.
wavs/20211022_144505_95.wav|If false positive raised from zero to let's say thirtyfive, A is better than B.
wavs/20211022_144505_96.wav|since A curves is over the B curves, okay?
wavs/20211022_144505_97.wav|Let's know that the ideal curve has this point.
wavs/20211022_144505_98.wav|Since in this point all same, all the true positive rate is one hundred and the first positive rate is zero, okay?
wavs/20211022_144505_99.wav|So an ideal curve is this one?
wavs/20211022_144505_100.wav|Since I should have only this point in the ideal case.
wavs/20211022_144505_101.wav|One while first positive is equal to zero, okay?
wavs/20211022_144505_102.wav|So sorry.
wavs/20211022_144505_103.wav|Even if we have a quiet I post positive in this case B is better than A, okay?
wavs/20211022_144505_104.wav|So we have to choose by seeing these curves.
wavs/20211022_144505_105.wav|It can be demonstrate that we can build by considering these two records.
wavs/20211022_144505_106.wav|We can build a new classifier that perform always in the best possible way.
wavs/20211022_144505_107.wav|And said that it follows A in this part, B in this part.
wavs/20211022_144505_108.wav|And it's also a bowl to follow this line in between, okay?
wavs/20211022_144505_109.wav|So it is possible to evaluate and to construct the so called convex.
wavs/20211022_144505_110.wav|I'll the convex all of these, Of these curves, okay?
wavs/20211022_144505_111.wav|Area under A U C.
wavs/20211022_144505_112.wav|Area under the ROC curve.
wavs/20211022_144505_113.wav|Obviously the best possible value is equal to one.
wavs/20211022_144505_114.wav|I think if we consider that one hundred is equal to one, we have a square, okay?
wavs/20211022_144505_115.wav|One per one square.
wavs/20211022_144505_116.wav|So the area is equal to one.
wavs/20211022_144505_117.wav|So the best possible classifier as ALC equal to one, okay?
wavs/20211022_144505_118.wav|Here is the area under the ROC curve.
wavs/20211022_144505_119.wav|I'll see, okay?
wavs/20211022_144505_120.wav|Other few words and then we stop for a few minutes.
wavs/20211022_144505_121.wav|You are the definition of precision recall.
wavs/20211022_144505_122.wav|We can also have a curve name of the president recalled curve which is similar to drug curve.
wavs/20211022_144505_123.wav|Here is the product of sensitivity by specificity.
wavs/20211022_144505_124.wav|But once again we can use separately sensitivity specificity in case of medical domain, okay?
wavs/20211022_144505_125.wav|Sensitivity is the ability of pick up a specific disease for example.
wavs/20211022_144505_126.wav|And specificity is related to the false alarm in other words.
wavs/20211022_144505_127.wav|So they are also related to recall for example, okay?
wavs/20211022_144505_128.wav|It says that as you can see, sensitivity is equal to recall through positive divided by true positive plus false negative.
wavs/20211022_144505_129.wav|So recall and sensitivity is the same.
wavs/20211022_144505_130.wav|There is a summary of some measure.
wavs/20211022_144505_131.wav|So we can have President recall which typically uses information retrieval.
wavs/20211022_144505_132.wav|We don't say anything about lift trust but anyway it's not approved, okay?
wavs/20211022_144505_133.wav|So I have to we can use a small break.
wavs/20211022_144505_134.wav|And then we can start again in about seven minutes.
wavs/20211022_144505_135.wav|So in or less twenty to twentyfour, okay.
wavs/20211022_144505_136.wav|I stop recording for a while and then I'll start again when we have to to continue.
wavs/20211020_144056_0.wav|Typically, they don't change in a dramatic way.
wavs/20211020_144056_1.wav|But anyway, it changes if possible.
wavs/20211020_144056_2.wav|Anyway, I'll illustrate this procedure in a deeper detail.
wavs/20211020_144056_3.wav|I can find that system A, for example, performs better than system B.
wavs/20211020_144056_4.wav|The improvement that I can have when using system A is a real improvement.
wavs/20211020_144056_5.wav|Well, by using this approach, I can also compare different machine learning schemes.
wavs/20211020_144056_6.wav|Since the cost of a false typically can be the need of another analysis.
wavs/20211020_144056_7.wav|But anyway, the cost is limited.
wavs/20211020_144056_8.wav|But I can also use costs during the training.
wavs/20211020_144056_9.wav|It says that if I have a set of probabilities for different classes.
wavs/20211020_144056_10.wav|If a classifier outputs a probability for each class.
wavs/20211020_144056_11.wav|Well, by considering costs, I can change the decision of the classifier.
wavs/20211020_144056_12.wav|But anyway, it is also possible.
wavs/20211020_144056_13.wav|Well, maybe the description length is a possible solution.
wavs/20211020_144056_14.wav|This principle is obviously more general.
wavs/20211020_144056_15.wav|It's not only possible to use MDL, minimum description length in case of kmeans.
wavs/20211020_144056_16.wav|It says that if, for example, I have to consider a nearest neighbor classifier.
wavs/20211020_144056_17.wav|Obviously, old instances are correct classified.
wavs/20211020_144056_18.wav|If I build a tree with a number of leaves, which is equal to the number of instances to the training set.
wavs/20211020_144056_19.wav|Also, in that case, I can correctly classify all my training data.
wavs/20211020_144056_20.wav|Another problem.
wavs/20211020_144056_21.wav|I'll also illustrate it in more details.
wavs/20211020_144056_22.wav|But anyway, I think that it's better to underline this concept.
wavs/20211020_144056_23.wav|On the other hand, if I'm able to correctly train a system, I must be able also to properly assess its performance.
wavs/20211020_144056_24.wav|Then I need to make a smart use of all my data.
wavs/20211020_144056_25.wav|If I have to assess the performance of a system, I have to fix it to define the performance measure.
wavs/20211020_144056_26.wav|Again, consider number of correct classifications or also a number of errors.
wavs/20211020_144056_27.wav|It's obvious that there is a direct relation between these two measures.
wavs/20211020_144056_28.wav|I can use number of correct classifications, the so called accuracy of a classifier.
wavs/20211020_144056_29.wav|For example, miss detections and false alarms can have different cause.
wavs/20211020_144056_30.wav|Error rate is the proportion of errors on the worst set of instances that have to be classified.
wavs/20211020_144056_31.wav|A resubstitution error is typically optimistic.
wavs/20211020_144056_32.wav|By resubstitution error is optimistic, I have to underestimates the real error rate.
wavs/20211020_144056_33.wav|A key point is that such instances have played no part in formation of classifier.
wavs/20211020_144056_34.wav|I want to underline these words.
wavs/20211020_144056_35.wav|They have played no part in formation of classifier.
wavs/20211020_144056_36.wav|Why? This concept it very important.
wavs/20211020_144056_37.wav|The fact is that why do we have to train a classifier?
wavs/20211020_144056_38.wav|For example let's think to the perceptron.
wavs/20211020_144056_39.wav|In that case, typically, once again, I can have an overestimation of the performance.
wavs/20211020_144056_40.wav|Since typically, a learning scheme operates in two different stages.
wavs/20211020_144056_41.wav|This is the procedure.
wavs/20211020_144056_42.wav|I have to provide you a training set.
wavs/20211020_144056_43.wav|Use test set for assessing performance in order to choose the best possible solution among all the possible ones.
wavs/20211020_144056_44.wav|You can apply one of the classifiers I illustrated.
wavs/20211020_144056_45.wav|You can use one of the classifiers I illustrated.
wavs/20211020_144056_46.wav|Then only that day we'll be able to see the real performance on test data of your system.
wavs/20211020_144056_47.wav|You have to submit a solution.
wavs/20211020_144056_48.wav|In principle, you can submit different solutions.
wavs/20211020_144056_49.wav|The system, it can choose the best one, or you can select this is my preferred solution.
wavs/20211020_144056_50.wav|This way, you can have an idea of the performance of your system.
wavs/20211020_144056_51.wav|You can use this value in order to see if it is below the threshold or not.
wavs/20211020_144056_52.wav|Obviously, another important point is to assess the performance of your system in a proper way.
wavs/20211020_144056_53.wav|More in general, I'm spending other five minutes on this point.
wavs/20211020_144056_54.wav|But also a very large dataset so that also the assessment of the performance is quite good.
wavs/20211020_144056_55.wav|In other words, in an ideal case, both training set and the test set should be quite large.
wavs/20211020_144056_56.wav|After that, training can be splitted once again into training and validation.
wavs/20211020_144056_57.wav|But at least in principle, we need two sets, training and test.
wavs/20211020_144056_58.wav|In case of the holdout method, we have to reserve a part of data for testing.
wavs/20211020_144056_59.wav|Typically, I can use two third for training and one third for testing.
wavs/20211020_144056_60.wav|A typical split will be twothirds for training and one third for testing.
wavs/20211020_144056_61.wav|If I split data independently of the class, I can have some problems.
wavs/20211020_144056_62.wav|For example, if you remember Iris data.
wavs/20211020_144056_63.wav|In case of Iris data, we have one hundred and fifty instances, fifty for each class.
wavs/20211020_144056_64.wav|This is not a good choice, It's obvious.
wavs/20211020_144056_65.wav|A possible solution is to use the socalled the stratification.
wavs/20211020_144056_66.wav|It's the same. It doesn't matter.
wavs/20211020_144056_67.wav|This way, proportion are the same.
wavs/20211020_144056_68.wav|These are the so called repeated holdout method.
wavs/20211020_144056_69.wav|Anyway, I'll present to you after the break the crossvalidation approach.
wavs/20211020_144056_70.wav|Crossvalidation is a possible answer to this particular aim.
wavs/20211020_144056_71.wav|seventyeight, mute.
wavs/20211020_144056_72.wav|Brat, no more, thanks.
wavs/20211020_144056_73.wav|four thousand, three hundred and fortyfour, I have to start again.
wavs/20211020_154514_0.wav|What is the answer?
wavs/20211020_154514_1.wav|Well, a possible solution is to use the socalled kfold crossvalidation.
wavs/20211020_154514_2.wav|When I'm using kfold crossvalidation, I have to split the old data into k folds.
wavs/20211020_154514_3.wav|For example, let's pick k equal to ten.
wavs/20211020_154514_4.wav|Then I can perform ten different trainings.
wavs/20211020_154514_5.wav|For example, for the first training, I'm considering folds from one to nine for training, and the last one for test.
wavs/20211020_154514_6.wav|At each training, I use a different fold for testing.
wavs/20211020_154514_7.wav|In this case, I have to apply learning algorithm to k different training sets.
wavs/20211020_154514_8.wav|If k is equal to ten, I have to perform ten trainings.
wavs/20211020_154514_9.wav|This approach is called leaveoneout.
wavs/20211020_154514_10.wav|Since, I leave one instance out for each test.
wavs/20211020_154514_11.wav|This is the so called leaveoneout.
wavs/20211020_154514_12.wav|If I have n training instances, I've to build n classifiers.
wavs/20211020_154514_13.wav|An advantage is that we have to perform normal random subsampling.
wavs/20211020_154514_14.wav|It makes use all data in the best possible way.
wavs/20211020_154514_15.wav|The drawback of this approach is given by its computational cost.
wavs/20211020_154514_16.wav|For small datasets, leaveoneout is a possible solution.
wavs/20211020_154514_17.wav|Bootstrap instead uses sampling with replacement in order to form training set.
wavs/20211020_154514_18.wav|A specific instance can be present more than one time into the training.
wavs/20211020_154514_19.wav|The number of instances is the same with respect to our dataset.
wavs/20211020_154514_20.wav|This instances will build the test site.
wavs/20211020_154514_21.wav|The probability that a specific instance will belong to the pesticide is zero point three six eight.
wavs/20211020_154514_22.wav|In other words, training data.
wavs/20211020_154514_23.wav|The training site will contain more or less seventythree percent of the all instances.
wavs/20211020_154514_24.wav|As you can see, this number is quite similar to twothree.
wavs/20211020_154514_25.wav|Two third is sixtysix percent.
wavs/20211020_154514_26.wav|In this case I have sixtythree percent.
wavs/20211020_154514_27.wav|By using Bootstrap, more or less, the test set is one third, with respect to the original data.
wavs/20211020_154514_28.wav|The possible approach is for performing the evaluation of the performance.
wavs/20211020_154514_29.wav|Generally speaking, if our dataset is very huge, holdout veto can be good.
wavs/20211020_154514_30.wav|Anyway, it's always better to use grading and validation.
wavs/20211020_154514_31.wav|They find the structure of the classifier and validation in order to select the parameter.
wavs/20211020_154514_32.wav|An example is k. In case of a knearest neighbor classifier.
wavs/20211020_154514_33.wav|The procedure is the same as I already explained.
wavs/20211020_154514_34.wav|Once we fix it, a parameter or an hyperparameter.
wavs/20211020_154514_35.wav|In order to inaudible, I'll split the data into subsets.
wavs/20211020_154514_36.wav|A possibility is to use the socalled lead nested crossvalidation.
wavs/20211020_154514_37.wav|While the inner crossvalidation is used to choose hyperparameter values.
wavs/20211020_154514_38.wav|Either cross validations are obviously part of the learning process.
wavs/20211020_154514_39.wav|Times, the impulse. Eight are correctly classified, that eighty percent is the the accuracy.
wavs/20211020_154514_40.wav|If there is a change on just one same book, the accuracy can become ninety percent or seventy percent.
wavs/20211020_154514_41.wav|In the sense that in addition to the prediction that I made.
wavs/20211020_154514_42.wav|Performance will be within the range.
wavs/20211020_154514_43.wav|For example, twentyminus thirty.
wavs/20211020_154514_44.wav|I cannot say it.
wavs/20211020_154514_45.wav|I can say that the real performance will be in a certain range.
wavs/20211020_154514_46.wav|Well, in that case, we have to enlarge the range.
wavs/20211020_154514_47.wav|In this case, I'm considering the accuracy or success rate.
wavs/20211020_154514_48.wav|If I measured that the estimated success rate is seventyfive percent.
wavs/20211020_154514_49.wav|The range goes from seventythree point two and seventysix point seven.
wavs/20211020_154514_50.wav|Well, if the performance is the same, seventyfive percent.
wavs/20211020_154514_51.wav|It is quite intuitive this is approach.
wavs/20211020_154514_52.wav|The width of the interval is more or less or twentyone points.
wavs/20211020_154514_53.wav|In this case, the width of the interval is three point five.
wavs/20211020_154514_54.wav|From three point five to eleven.
wavs/20211020_154514_55.wav|Is it eleven or twentyone? Is eleven.
wavs/20211020_154514_56.wav|From three point five to eleven.
wavs/20211020_154514_57.wav|In this case, we have an estimation, one samples.
wavs/20211020_154514_58.wav|Now we have an estimation of only one hundred samples.
wavs/20211020_154514_59.wav|Well, here is the whole procedure for determining the confidence intervals.
wavs/20211020_154514_60.wav|We have to fix the confidence that we want.
wavs/20211020_154514_61.wav|For example, we want a prediction with a confidence of eighty percent.
wavs/20211020_154514_62.wav|The probability that X is in this range is ninety percent.
wavs/20211020_154514_63.wav|one hundred minus ninety is equal to ten, divided by two is equal to five, and then I have to pick this value for z.
wavs/20211020_154514_64.wav|Confidence value is expressed as a percentage.
wavs/20211020_154514_65.wav|F is the value I obtain on my data.
wavs/20211020_154514_66.wav|In this case, f is equal to seventyfive percent.
wavs/20211020_154514_67.wav|F is the accuracy of my classifier.
wavs/20211020_154514_68.wav|I'm subtracting the mean and then dividing by the standard deviation.
wavs/20211020_154514_69.wav|This is a transformed value for f.
wavs/20211020_154514_70.wav|c is the value I'm to fix, c for example, is ninety percent, ninetyfive percent, ninetynine percent.
wavs/20211020_154514_71.wav|This value of z must be used in this formula.
wavs/20211020_154514_72.wav|If I consider plus, I have the upper bound, when I consider minus, have the lower bound.
wavs/20211020_154514_73.wav|As you can see, n is at the denominator of these ratio.
wavs/20211020_154514_74.wav|The if is n, the lower is the confidence limits.
wavs/20211020_154514_75.wav|On the other hand, z is at the numerator of all these ratios.
wavs/20211020_154514_76.wav|The if is z, er, is the confidence dimension, the range of the confidence limits.
wavs/20211020_154514_77.wav|As you can see, the I is the probability, the I is the value of z.
wavs/20211020_154514_78.wav|Then range increases since that z is at the numerator of this ratio, this formula.
wavs/20211020_154514_79.wav|If number of test says that sets sample is lower than one hundred, we cannot use these tables.
wavs/20211020_154514_80.wav|But anyway, here are the real figures the actual figures.
wavs/20211020_154514_81.wav|In other words, I'm saying that all results are possible.
wavs/20211020_154514_82.wav|It doesn't make sense.
wavs/20211020_154514_83.wav|It should be taken with a grain of salt inaudible in this slide.
wavs/20211020_154514_84.wav|In that case, I have to fix the confidence value I want.
wavs/20211020_154514_85.wav|I have to fix the probability for which the real value must be into a specific range.
wavs/20211020_154514_86.wav|The I is number of same post.
wavs/20211020_154514_87.wav|Lower will be the range.
wavs/20211020_154514_88.wav|The value I've evaluated.
wavs/20211020_154514_89.wav|Classifier A is better than classifier B.
wavs/20211020_154514_90.wav|If I'm considering confidence interval, if they don't overlap, that's okay.
wavs/20211020_154514_91.wav|With a probability of eightynineminus ninetyfive percent classifier A is always better than classifier B.
wavs/20211020_154514_92.wav|It's a good choice, classifier A.
wavs/20211020_154514_93.wav|It's not correct to say that classifier A is better than classifier B.
wavs/20211020_154514_94.wav|This can be done by following this approach.
wavs/20211020_154514_95.wav|They are not totally independent.
wavs/20211020_154514_96.wav|Obviously, if I have, once again, an infinite number of samples, I can do this comparison.
wavs/20211020_154514_97.wav|We can use a paired ttest that if individual samples are paired, what does it mean?
wavs/20211020_154514_98.wav|Embracing all this can be always true.
wavs/20211020_154514_99.wav|I'll note that in some cases, this is not possible.
wavs/20211020_154514_100.wav|I don't have the exact distribution.
wavs/20211020_154514_101.wav|I know that they performed a stratified ten fold crossvalidation.
wavs/20211020_154514_102.wav|In that case, I have to use the socalled unpaired ttest.
wavs/20211020_154514_103.wav|By using a distribution which is very similar to the normal distribution we use the inaudible confidence limit.
wavs/20211020_154514_104.wav|Here is the way in which I can use this faster, I repeat on Friday.
wavs/20211020_154514_105.wav|In case of a great observation, I have to slightly modify the test from this one to this one.
wavs/20211020_154514_106.wav|In case of dependent estimation, when I cannot have an infinite number of Data.
wavs/20211020_154514_107.wav|In other words, this is the real case.
wavs/20211020_154514_108.wav|In this case, I have to use the socalled corrected sample ttest.
wavs/20211020_154514_109.wav|The difference between the performance over two system is statistically significant or not.
wavs/20211020_154514_110.wav|If the difference isn't statistically significant.
wavs/20211020_154514_111.wav|This means that to classify are practically the same.
